import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler 


def Sigmoid(z):
    return 1 / (1 + np.exp(-z))

def Loss(Y, Y_hat):
    n = Y.shape[0]
    eps = 1e-9

    if len(Y.shape) > 1: Y = Y.flatten()
    if len(Y_hat.shape) > 1: Y_hat = Y_hat.flatten()
    
    loss = -1 / n * np.sum(Y * np.log(Y_hat + eps) + (1 - Y) * np.log(1 - Y_hat + eps))
    return loss

def GD(X, Y, Y_hat):
    n = X.shape[0]
  
    dW = 1 / n * np.dot(X.T, (Y_hat.reshape(-1, 1) - Y.reshape(-1, 1)))
    dc = 1 / n * np.sum(Y_hat - Y)
    return dW.flatten(), dc

def Train_LR(X, Y, lr=0.001, ep=10000):
    X = np.array(X, dtype=float)
    Y = np.array(Y, dtype=float).reshape(-1, 1)

    W = np.zeros((X.shape[1], 1))
    c = 0.0
    losses_history = []
    
    for i in range(ep):
        Z = np.dot(X, W) + c
        Y_hat = Sigmoid(Z)
        
        if (i % (ep // 100) == 0) or (i == ep - 1):
             losses_history.append(Loss(Y, Y_hat))
        
        dW, dc = GD(X, Y.flatten(), Y_hat.flatten())
        
        W -= lr * dW.reshape(-1, 1) 
        c -= lr * dc

    return W.flatten(), c, np.array(losses_history)

def Hybrid_Predict_Proba(X_input, rf_model, W, c):
    X = np.array(X_input, dtype=float)

    Z_lr = np.dot(X, W.reshape(-1, 1)) + c
    Y_hat_lr = Sigmoid(Z_lr).flatten()

    Y_hat_rf = rf_model.predict_proba(X)[:, 1]

    Y_pred_proba = 0.6 * Y_hat_lr + 0.4 * Y_hat_rf

    return Y_pred_proba

def Final_Predict(Y_prob, threshold=0.5):

    Y_class = np.where(Y_prob > threshold, 1, 0) 
    return Y_class

@st.cache_resource
def get_trained_model_results():
    try:
        fl = pd.read_csv("./Loan - Loan.csv") 
    except FileNotFoundError:
        st.error("L·ªói: Kh√¥ng t√¨m th·∫•y file Loan - Loan.csv. Vui l√≤ng ƒë·∫£m b·∫£o file n·∫±m c√πng th∆∞ m·ª•c.")
        return None, None, None, None, None, None, None, None, None, None 
        
    fl['education'] = fl['education'].map({'Graduate': 1, 'Not Graduate': 0})
    fl['self_employed'] = fl['self_employed'].map({'Yes': 1, 'No': 0})
    fl['loan_status'] = fl['loan_status'].map({'Approved': 1, 'Rejected': 0})
    
    FEATURE_NAMES = list(fl.columns[1:-1])
    X_data = fl[FEATURE_NAMES].values.astype(float)
    Y_data = fl['loan_status'].values.astype(int)

    scaler = StandardScaler()
    X_data_scaled = scaler.fit_transform(X_data) 

    X_train, _, Y_train, _ = train_test_split(
        X_data_scaled, Y_data, test_size=0.4, random_state=11, stratify=Y_data
    )

    rf_uncalibrated = RandomForestClassifier(n_estimators=1000, random_state=11, n_jobs=-1)
    rf_model = CalibratedClassifierCV(rf_uncalibrated, method="sigmoid", cv=5)
    rf_model.fit(X_train, Y_train)

    W_lr, c_lr, losses_history = Train_LR(X_data_scaled, Y_data, ep=10000)
 
    Y_prob_full = Hybrid_Predict_Proba(X_data_scaled, rf_model, W_lr, c_lr)
    Y_pred_full = Final_Predict(Y_prob_full)
    
    return FEATURE_NAMES, X_data_scaled, Y_data, W_lr, c_lr, losses_history, Y_pred_full, Y_prob_full, rf_model, scaler

FEATURE_NAMES, x, y, w, b, losses, y_pred, y_prob, rf_model, scaler = get_trained_model_results()

if FEATURE_NAMES is None:
    st.stop()

FEATURE_VIETMAP = {
    "no_of_dependents": "S·ªë ng∆∞·ªùi ph·ª• thu·ªôc", "education": "Tr√¨nh ƒë·ªô h·ªçc v·∫•n", 
    "self_employed": "T·ª± kinh doanh", "income_annum": "Thu nh·∫≠p h·∫±ng nƒÉm", 
    "loan_amount": "S·ªë ti·ªÅn vay", "loan_term": "Th·ªùi h·∫°n vay", 
    "cibil_score": "ƒêi·ªÉm t√≠n d·ª•ng (CIBIL)", "residential_assets_value": "Gi√° tr·ªã t√†i s·∫£n nh√† ·ªü", 
    "commercial_assets_value": "Gi√° tr·ªã t√†i s·∫£n kinh doanh", 
    "luxury_assets_value": "Gi√° tr·ªã t√†i s·∫£n cao c·∫•p", "bank_asset_value": "T√†i s·∫£n t·∫°i ng√¢n h√†ng"
}


def predict_labels(X_input_raw, W, c, rf_model_used, scaler_used):
    """S·ª≠ d·ª•ng m√¥ h√¨nh hybrid ƒë·ªÉ d·ª± ƒëo√°n cho ƒëi·ªÉm d·ªØ li·ªáu m·ªõi."""

    X_scaled_new = scaler_used.transform(X_input_raw) 
    
    P = Hybrid_Predict_Proba(X_scaled_new, rf_model_used, W, c)
    return Final_Predict(P), P

st.set_page_config(layout="wide", page_title="Dashboard T√≠n D·ª•ng", page_icon="üè¶")


with st.sidebar:
    st.title("üè¶ H·ªá Th·ªëng Duy·ªát H·ªì S∆° Vay")
    st.markdown("Nh√≥m 2 - Gi·∫£i t√≠ch 1")

    st.header("Tham s·ªë m√¥ h√¨nh")
    with st.expander("Xem tr·ªçng s·ªë v√† bias"):
        st.metric(label="Bias (c)", value=f"{b:.4f}")
        for feature, weight in zip(FEATURE_NAMES, w):
            st.markdown(f"**{FEATURE_VIETMAP[feature]}**: {weight:.4f}")

    st.header("Nh·∫≠p h·ªì s∆° c·∫ßn ki·ªÉm tra")
    input_data_raw = {}
 
    X_data_unscaled = scaler.inverse_transform(x) 
    mean_x_unscaled = np.mean(X_data_unscaled, axis=0)

    for i, feature in enumerate(FEATURE_NAMES):
        label_vi = FEATURE_VIETMAP[feature]
        default_value_unscaled = mean_x_unscaled[i]

        if feature == "no_of_dependents":
            input_data_raw[feature] = st.number_input(label=label_vi, min_value=0, step=1, value=int(round(default_value_unscaled)))
        elif feature == "education":
            current_choice = "T·ªët nghi·ªáp" if default_value_unscaled > 0.5 else "Ch∆∞a t·ªët nghi·ªáp"
            input_data_raw[feature] = st.selectbox(label=label_vi, options=["T·ªët nghi·ªáp", "Ch∆∞a t·ªët nghi·ªáp"], index=["T·ªët nghi·ªáp", "Ch∆∞a t·ªët nghi·ªáp"].index(current_choice))
        elif feature == "self_employed":
            current_choice = "C√≥" if default_value_unscaled > 0.5 else "Kh√¥ng"
            input_data_raw[feature] = st.selectbox(label=label_vi, options=["C√≥", "Kh√¥ng"], index=["C√≥", "Kh√¥ng"].index(current_choice))
        else:
            value_to_display = int(default_value_unscaled) if default_value_unscaled > 10000 or feature in ['cibil_score', 'loan_term'] else float(default_value_unscaled)
            input_data_raw[feature] = st.number_input(
                label=label_vi,
                value=value_to_display,
                format="%d" if default_value_unscaled > 10000 or feature in ['cibil_score', 'loan_term'] else "%.2f"
            )

    if st.button("D·ª± ƒëo√°n h·ªì s∆°", use_container_width=True, type="primary"):

        final_input_values_raw = []
        for name in FEATURE_NAMES:
            if name == 'education':
                final_input_values_raw.append(1.0 if input_data_raw[name] == "T·ªët nghi·ªáp" else 0.0)
            elif name == 'self_employed':
                final_input_values_raw.append(1.0 if input_data_raw[name] == "C√≥" else 0.0)
            else:
                final_input_values_raw.append(float(input_data_raw[name]))
        
        new_point_raw = np.array(final_input_values_raw).reshape(1, -1)

        prediction, prob = predict_labels(new_point_raw, w, b, rf_model, scaler)

        if prediction[0] == 1:
            st.success(f"‚úÖ K·∫øt qu·∫£: **DUY·ªÜT H·ªí S∆†** ‚Äî X√°c su·∫•t: {prob[0]:.2%}")
            st.balloons()
        else:
            st.error(f"‚ùå K·∫øt qu·∫£: **T·ª™ CH·ªêI** ‚Äî X√°c su·∫•t b·ªã t·ª´ ch·ªëi: {(1 - prob[0]):.2%}")


tab1, tab2, tab3 = st.tabs(["Qu√° tr√¨nh hu·∫•n luy·ªán", "ƒê√°nh gi√° m√¥ h√¨nh", "Tr·ª±c quan h√≥a"])

EP_TRAIN = 10000 
loss_divisor = 100 
loss_indices = np.arange(len(losses)) * (EP_TRAIN // loss_divisor) 


with tab1:
    st.header("Di·ªÖn bi·∫øn Loss")
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("D·ªØ li·ªáu ƒë·∫ßu v√†o m√¥ h√¨nh")
        df_scaled = pd.DataFrame(x, columns=[FEATURE_VIETMAP[f] for f in FEATURE_NAMES])
        df_scaled['Tr·∫°ng th√°i vay (y)'] = y
        st.dataframe(df_scaled.head())

    with col2:
        st.subheader("Loss gi·∫£m theo th·ªùi gian (Logistic Regression)")
        fig, ax = plt.subplots()
        ax.plot(loss_indices, losses, color='blue')
        ax.set_title("Loss gi·∫£m theo Epoch ")
        ax.set_xlabel("Epoch (x100 iterations)")
        ax.set_ylabel("Loss (Binary Cross-Entropy)")
        st.pyplot(fig)

with tab2:
    st.header("Hi·ªáu nƒÉng m√¥ h√¨nh")
    col1, col2 = st.columns([1, 1.4])
    
    if len(y_pred) > 0:
        with col1:
            acc = accuracy_score(y, y_pred)
            st.metric("ƒê·ªô ch√≠nh x√°c", f"{acc:.2%}")

            report = classification_report(
                y, y_pred,
                target_names=['T·ª´ ch·ªëi', 'Duy·ªát'],
                output_dict=True
            )
            st.subheader("B√°o c√°o ph√¢n lo·∫°i")
            st.dataframe(pd.DataFrame(report).transpose())

        with col2:
            st.subheader("Ma tr·∫≠n Nh·∫ßm l·∫´n")
            cm = confusion_matrix(y, y_pred)
            fig_cm, ax_cm = plt.subplots()
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                        xticklabels=['D·ª± ƒëo√°n: T·ª´ ch·ªëi', 'D·ª± ƒëo√°n: Duy·ªát'],
                        yticklabels=['Th·ª±c t·∫ø: T·ª´ ch·ªëi', 'Th·ª±c t·∫ø: Duy·ªát'],
                        ax=ax_cm)
            ax_cm.set_title('Confusion Matrix')
            st.pyplot(fig_cm)
    else:
        st.warning("Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√° hi·ªáu nƒÉng.")


with tab3:
    st.header("Tr·ª±c quan h√≥a 2 chi·ªÅu")
    st.warning("Do d·ªØ li·ªáu c√≥ 11 chi·ªÅu, ch·ªâ c√≥ th·ªÉ v·∫Ω 'l√°t c·∫Øt' 2D b·∫±ng c√°ch ch·ªçn 2 ƒë·∫∑c tr∆∞ng.", icon="‚ö†Ô∏è")

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ƒê·ªô quan tr·ªçng c·ªßa ƒë·∫∑c tr∆∞ng (LR Weights)")
        importance = pd.DataFrame({
            "ƒê·∫∑c tr∆∞ng": [FEATURE_VIETMAP[f] for f in FEATURE_NAMES],
            "ƒê·ªô quan tr·ªçng": np.abs(w)
        }).sort_values("ƒê·ªô quan tr·ªçng", ascending=False)

        fig_imp, ax_imp = plt.subplots(figsize=(10, 6))
        sns.barplot(x="ƒê·ªô quan tr·ªçng", y="ƒê·∫∑c tr∆∞ng", data=importance, palette="viridis", ax=ax_imp)
        ax_imp.set_title("Tr·ªçng s·ªë (W) tuy·ªát ƒë·ªëi c·ªßa Logistic Regression")
        st.pyplot(fig_imp)

    with col2:
        st.subheader("Ch·ªçn hai ƒë·∫∑c tr∆∞ng ƒë·ªÉ v·∫Ω ranh gi·ªõi")
        

        format_func_vi = lambda f: FEATURE_VIETMAP[f]

        safe_index_cibil = FEATURE_NAMES.index('cibil_score') if 'cibil_score' in FEATURE_NAMES else 0
        safe_index_income = FEATURE_NAMES.index('income_annum') if 'income_annum' in FEATURE_NAMES else 1

        feat_x = st.selectbox("Tr·ª•c X ", FEATURE_NAMES, index=safe_index_cibil, format_func=format_func_vi)
        feat_y = st.selectbox("Tr·ª•c Y", FEATURE_NAMES, index=safe_index_income, format_func=format_func_vi)

        if feat_x == feat_y:
            st.error("Hai ƒë·∫∑c tr∆∞ng ph·∫£i kh√°c nhau.")
        else:
            ix = FEATURE_NAMES.index(feat_x)
            iy = FEATURE_NAMES.index(feat_y)

            mean_values_scaled = np.mean(x, axis=0)
            grid = np.ones((100*100, len(FEATURE_NAMES))) * mean_values_scaled

            xr = np.linspace(x[:, ix].min(), x[:, ix].max(), 100)
            yr = np.linspace(x[:, iy].min(), x[:, iy].max(), 100)
            xx, yy = np.meshgrid(xr, yr)

            grid[:, ix] = xx.ravel()
            grid[:, iy] = yy.ravel()

            y_grid_prob = Hybrid_Predict_Proba(grid, rf_model, w, b)
            Z = Final_Predict(y_grid_prob)
            Z = Z.reshape(xx.shape)

            fig2, ax2 = plt.subplots(figsize=(10, 8))
            ax2.contourf(xx, yy, Z, alpha=0.25, cmap=plt.cm.coolwarm)

            sns.scatterplot(
                x=x[:, ix], y=x[:, iy], hue=y,
                palette=['#FF5733', '#1F77FF'], 
                s=110, ax=ax2, style=y,
                legend='full'
            )
            ax2.set_xlabel(f"{FEATURE_VIETMAP[feat_x]} (Chu·∫©n h√≥a)")
            ax2.set_ylabel(f"{FEATURE_VIETMAP[feat_y]} (Chu·∫©n h√≥a)")

            st.pyplot(fig2)